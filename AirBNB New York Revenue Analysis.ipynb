{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3da09ef-049d-4f18-affb-8170c13913ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# AirBNB New York Revenue Analysis  <font size=2>v2.0</font>\n",
    "  <code style=\"color:#4a304f\"><font color= #d689e6>bootcamp project</font></code>\n",
    "\n",
    "### üíÅ‚Äç‚ôÄÔ∏è Team\n",
    "\n",
    "- [Gabrielle Rosa](https://github.com/gabxrosa)\n",
    "- [Nayara Ramos](https://github.com/nay-ramos)\n",
    "\n",
    "---\n",
    "\n",
    "### üóÉÔ∏è Dataset\n",
    "\n",
    "><font size=2>>>> [NY Airbnb open data - Kaggle.com](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data) by  Dgomonov  **and** [NY Airbnb listing details - Inside Airbnb](https://insideairbnb.com/get-the-data/) by Inside Airbnb</font>\n",
    "\n",
    "\n",
    ">\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùì‚ùì The question:\n",
    "\n",
    "1. What are the main factors (accommodation characteristics and host profile) that drive revenue from Airbnb listings in New York, and how can these insights be translated into effective strategies to increase the platform's revenue in the city?\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "###  ‚öôÔ∏è Methodology:\n",
    "  1. Data Consolidation\n",
    "  2. Data Selection\n",
    "  3. Database normalization\n",
    "  4. Understanding and cleaning the data\n",
    "  5. Preparing dataframe for PowerBI\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### üî® Tools\n",
    "\n",
    "<img width=150 title=\"Databricks\" alt=\"Databricks\" src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Flogos-world.net%2Fwp-content%2Fuploads%2F2024%2F01%2FDatabricks-Logo.jpg&f=1&nofb=1&ipt=1bbae2ef5109af0db178c7b5f304ccea32bf5c8081ac43a065ca1e34cf8de7b4\"> <img width=198 title=\"PowerBI\" alt=\"PowerBI\" src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fdatascientest.com%2Fes%2Ffiles%2F2020%2F10%2Fpower-bi-logo-1.jpg&f=1&nofb=1&ipt=099003c0f01745db97361ebe56ede7ec363ea05c5cf7a77759c528d984242c15\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38509096-a6d6-4c08-9842-c1c397d65338",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Project setup & data acquisition"
    }
   },
   "outputs": [],
   "source": [
    "# importing libs\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# spark session\n",
    "spark = SparkSession.builder.appName(\"spk\").getOrCreate()\n",
    "\n",
    "# reading data from airbnb listings in New York without infering schema\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .option(\"multiLine\", \"true\") \\\n",
    "    .csv('/Volumes/workspace/default/primeirovolume/airbnb_NYC_2025.csv')\n",
    "\n",
    "# reading complementar data from airbnb listings New York without infering schema\n",
    "dfplus = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"false\") \\\n",
    "        .option(\"delimiter\", \",\") \\\n",
    "        .option(\"quote\", \"\\\"\") \\\n",
    "        .option(\"escape\", \"\\\"\") \\\n",
    "        .option(\"multiLine\", \"true\") \\\n",
    "        .csv('/Volumes/workspace/default/primeirovolume/Detailed Listings data_nyc_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d4b080-90ba-4844-b147-2560be81ab1d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Understanding the data"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize columns names, types and counting\n",
    "df.printSchema()\n",
    "dfplus.printSchema()\n",
    "\n",
    "# verify amount of rows\n",
    "print(\"=== rows amount ===\")\n",
    "print(\"df:\", df.count(),\"/ dfplus: \",dfplus.count())\n",
    "\n",
    "# renaming neighborhood_highlight colunm\n",
    "dfplus = dfplus.withColumnRenamed(\"NeighborhoodFALSEhighlights\", \"neighborhood_highlights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0664e672-d555-46b7-bae7-810e8af33fc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Data Consolidation\n",
    "\n",
    "Both source DataFrames contain identical record counts (36403 rows each). This 1:1 correspondence enables seamless integration without data loss. The goal of this stage is to merge the primary Dataframe (`df`) with the supplementary Dataframe (`dfplus`) into a single, comprehensive dataset. To ensure data integrity, this process involves three key steps:\n",
    "- **Verifying the join key (`id`)** to indentify and correct any mismatches between the two sources. \n",
    "- **Removing redudant columns** that are present in both Dataframes. \n",
    "- **Performing an inner join** to create the final unified Dataframe, (`df_bnb`.)\n",
    "\n",
    "\n",
    "\n",
    "Both source DataFrames contain identical record counts (36403 rows each). This 1:1 correspondence enables seamless integration without data loss.\n",
    "\n",
    "We will perform a horizontal combination of both datasets (`inner join`), creating a unified dataframe that preserves all available attributes while maintaining data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57b682ff-d3ec-4d8e-8ff9-ae1b8c01c221",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verifying the Join Key"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "The left_anti join is used as a diagnostic tool to confirm that every\n",
    " id in df has a corresponding match in dfplus, and vice-versa.\n",
    "'''\n",
    " \n",
    "# verifying if there are any rows with id not in df (anti join)\n",
    "only_df = df.join(dfplus, on=\"id\", how=\"left_anti\")\n",
    "only_dfplus = dfplus.join(df, on=\"id\", how=\"left_anti\")\n",
    "only_df.display()\n",
    "only_dfplus.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65cbd13d-ec05-49ee-b693-0c15ff46343a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Findings:** \n",
    "\n",
    "The anti-join revealed 7 ids that did not match. The investigation showed a formatting discrepancy:\n",
    "\n",
    "**df:** using full number  (ex.: 565252455567607000)\n",
    "\n",
    "**dfplus:** using cientific notation (ex.: 5,65252E+17)\n",
    "\n",
    "To resolve this, the mismatched `id`s in `dfplus` will be manually corrected to match the format used in `df`. After the correction, the anti-join is run again to confirm that there are zero mismatched rows.\n",
    "\n",
    "\n",
    "**Mismatching rows:**\n",
    "\n",
    "**df:** using full number  (ex.: 565252455567607000)\n",
    "**dfplus:** using cientific notation (ex.: 5,65252E+17)\n",
    "\n",
    "Since there is only 7 of them, we'll set both ids as `string` and replace them for the for the full number following `df`'s original schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c05afe39-9792-4403-9581-60dde9389879",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Correcting id discrepancies"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# converting dfplus id column to string\n",
    "dfplus = dfplus.withColumn(\"id\", f.col(\"id\").cast(\"string\"))\n",
    "\n",
    "# replacing failed conversion ids\n",
    "dfplus = dfplus.withColumn(\"id\",\n",
    "        f.when(f.col(\"id\") == \"5,65252E+17\", \"565252455567607000\")\n",
    "        .when(f.col(\"id\") == \"6,68902E+17\", \"668902255426418000\")\n",
    "        .when(f.col(\"id\") == \"8,05806E+17\", \"805805963320996000\")\n",
    "        .when(f.col(\"id\") == \"8,29274E+17\", \"829274057349888000\")\n",
    "        .when(f.col(\"id\") == \"8,70213E+17\", \"870212853386092000\")\n",
    "        .when(f.col(\"id\") == \"8,94142E+17\", \"894142017712115000\")\n",
    "        .when(f.col(\"id\") == \"9,81184E+17\", \"981183733840544000\")\n",
    "        .otherwise(f.col(\"id\")\n",
    " )\n",
    ")\n",
    "\n",
    "# verifying if there aren't any rows with id not in dfplus\n",
    "only_df = df.join(dfplus, on=\"id\", how=\"left_anti\")\n",
    "only_dfplus = dfplus.join(df, on=\"id\", how=\"left_anti\")\n",
    "only_df.display()\n",
    "only_dfplus.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "846505b4-df34-4a6d-9ee9-4bbd9ee42ee7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Removing duplicate columns"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    " Before merging, it's necessary to remove columns that exist in both DataFrames to avoid ambiguity.\n",
    " The intersection of both column sets is identified and these duplicate columns are dropped from dfplus.\n",
    "'''\n",
    "\n",
    "# identifying duplicate columns (ignoring id)\n",
    "cols_df = df.columns\n",
    "cols_dfplus = dfplus.columns\n",
    "\n",
    "# columns intersection\n",
    "dup_cols = set(cols_df).intersection(cols_dfplus)\n",
    "\n",
    "# ignore id\n",
    "dup_cols.remove(\"id\")\n",
    "\n",
    "# showing duplicate columns\n",
    "print(\"Duplicate columns:\", dup_cols)\n",
    "\n",
    "# dropping duplicate columns\n",
    "dfplus_no_dups = dfplus.drop(*dup_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "640e7541-c3e7-42c0-b730-ff0505643367",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760972487935}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_f58b6d09\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_75cffd4c\",\"enabled\":true,\"columnId\":\"has_availability\",\"dataType\":\"string\",\"filterType\":\"oneof\",\"filterValues\":[\"t\"],\"filterConfig\":{\"caseSensitive\":true}}],\"local\":false,\"updatedAt\":1760972509273}],\"syncTimestamp\":1760972509273}",
       "queryPlanFiltersBlob": "[{\"kind\":\"call\",\"function\":\"or\",\"args\":[{\"kind\":\"call\",\"function\":\"in\",\"args\":[{\"kind\":\"identifier\",\"identifier\":\"has_availability\"},{\"kind\":\"literal\",\"value\":\"t\",\"type\":\"string\"}]}]}]",
       "tableResultIndex": 0
      }
     },
     "title": "Dataframes horizontal merging"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "With the join key validated and redundant columns removed, an inner join\n",
    " is performed to create the final unified DataFrame, df_bnb.\n",
    "'''\n",
    "\n",
    "# ascending id classification\n",
    "print(\"df rows count:\", df.count())\n",
    "print(\"dfplus_no_dups rows count:\", dfplus_no_dups.count())\n",
    "df = df.orderBy(\"id\", ascend=True)\n",
    "dfplus_no_dups = dfplus_no_dups.orderBy(\"id\", ascend=True)\n",
    "\n",
    "# inner join df and dfplus_no_dups\n",
    "df_bnb = df.join(dfplus_no_dups, on=\"id\", how=\"inner\")\n",
    "print(\"df_bnb rows count:\", df_bnb.count())\n",
    "df_bnb.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9841f6eb-7f6b-42a2-ae45-0bdbeff272a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "print schema"
    }
   },
   "outputs": [],
   "source": [
    "df_bnb.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae5cec90-7f34-4f3a-ba95-bb5013d1721b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Type casting"
    }
   },
   "outputs": [],
   "source": [
    "# list of columns to cast to another type\n",
    "cast_types = {\n",
    "    'price': 'double',\n",
    "    'minimum_nights': 'int',\n",
    "    'maximum_nights': 'int',\n",
    "    'availability_365': 'int',\n",
    "    'number_of_reviews': 'int',\n",
    "    'review_scores_rating': 'double',\n",
    "    'review_scores_accuracy': 'double',\n",
    "    'review_scores_cleanliness': 'double',\n",
    "    'review_scores_checkin': 'double',\n",
    "    'review_scores_communication': 'double',\n",
    "    'review_scores_location': 'double',\n",
    "    'review_scores_value': 'double',\n",
    "    'number_of_reviews_ly': 'int',\n",
    "    'estimated_occupancy_l365d': 'int',\n",
    "    'number_of_reviews_ltm': 'int',\n",
    "    'number_of_reviews_l30d': 'int',\n",
    "    'last_review': 'date',\n",
    "    'first_review': 'date',\n",
    "    'bedrooms': 'int',\n",
    "    'acomodates': 'int',\n",
    "    'host_listings_count': 'int',\n",
    "    'host_total_listings_count': 'int',\n",
    "    'host_total_listings_count': 'int',\n",
    "    'host_since': 'date',\n",
    "    'host_response_time': 'string'\n",
    "}\n",
    " \n",
    "# casting types only on listed columns and keeping the other columns as they are\n",
    "df_bnb = df_bnb.select([\n",
    "    f.col(col).cast(cast_types[col]).alias(col) if col in cast_types \n",
    "    else f.col(col) \n",
    "    for col in df_bnb.columns\n",
    "])\n",
    "\n",
    "df_bnb.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91a4d507-2d76-4b6e-bec0-4f788f1190bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Data selection\n",
    "\n",
    "The primary goal of this analysis is to identify strategies for increasing Airbnb's revenue. To achieve this, we must first select the most relevant and high-quality data. This section will follow three main steps:\n",
    "- **Analyze the null value distribution** to understand data completeness.\n",
    "- **Define and isolate active listings** to focus on the most relevant segment of the market.\n",
    "- **Identify and remove columns** that are irrelevant to our analysis.\n",
    "\n",
    "\n",
    "Our analysis main goal is to raise Airbnb's revenue for the next year. To make it happen we need do to understand witch data is relevant to create an effective analysis. Here are the steps do define or priority:\n",
    "1. Understanding `Null` values distribution\n",
    "2. Discover active listings\n",
    "3. Remove columns that won't influence our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b139bbdb-4212-475b-bbdd-867e5ced3cdc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "def Mapping Null values"
    }
   },
   "outputs": [],
   "source": [
    "# function for null percentage analysis on a DataFrame\n",
    "def mappingNullvalues(df):\n",
    "    df_count = df.count()\n",
    "\n",
    "    null_info = []\n",
    "    for col in df.columns:\n",
    "        null_count = df.filter(f.col(col).isNull()).count()\n",
    "        null_percentage = (null_count / df_count) * 100\n",
    "        null_info.append((col, null_count, round(null_percentage, 2)))\n",
    "\n",
    "    # creating a new, sorted DataFrame to display the results clearly\n",
    "    schema = [\"column_name\", \"null_count\", \"null_percentage\"]\n",
    "    df_null_mapping = spark.createDataFrame(null_info, schema)\n",
    "\n",
    "    print(f\"Null value analysis for {df_count} total records:\")\n",
    "    display(df_null_mapping.orderBy(f.desc(\"null_percentage\")))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d29387d3-3c60-46a7-bd3b-ebc9ae0e177f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Null values analysis"
    }
   },
   "outputs": [],
   "source": [
    "# First, replace \"N/A\" strings with actual null values to ensure accurate counting\n",
    "df_bnb = df_bnb.replace(\"N/A\", None)\n",
    "\n",
    "# calculate null counts and percentages after setting N/A\n",
    "mappingNullvalues(df_bnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a89cf3a7-607e-4de6-b98a-7c0253dfdc15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Hypothesis on Missing Price\n",
    "\n",
    "With **41.55%** of `price`data missing, we hypothesize that the absence of a price is a indicator of an inactive listing. Given that the dataset contains historical data since 2008, it's plausible that many of these records without a price correspond to rooms that are no longer on the market. \n",
    "\n",
    "Since our goal is to inform revenue strategy for the upcoming year (2026), analyzing recent trends is more relevant than studying decade-old data.\n",
    "\n",
    "**Next Step:** To understand the relationship between price and room activity to determine a timeframe of data that makes sense for this analysis. Therefore, we will begin by investigating the characteristics of listings with `null` prices, using other activity indicators to test the hypothesis that they are inactive.\n",
    "\n",
    "#### Hypothesis on Missing Review Data\n",
    "\n",
    "A notable pattern is observed in the review-related columns, as many of them share a similar null rate of approximately **31%**. This is not necessarily a data quality issue, but rather an expected outcome for listings that have never received a review.\n",
    "\n",
    "We hypothesize two primary reasons for a listing having no reviews:\n",
    "1.  **New Listings:** The room was recently registered and has not yet had time to accumulate reviews.\n",
    "2.  **Inactive Listings:** The room is old and no longer active, thus receiving no new reviews.\n",
    "\n",
    "**Next Step:** To differentiate between these two cases and measure a listing's current activity level, we will use proxy metrics that are independent of reviews, such as `availability_365`, `number_of_reviews_ly`, and `estimated_occupancy_l365d`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ea2f301-b72b-406b-898d-5623c84633b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.1 Accommodation Analysis Based on Review Activity\n",
    "\n",
    "**Hypothesis:** A significant portion of missing values ‚Äã‚Äãin review-related columns (such as `review_scores_rating`, `last_review`, `reviews_per_month`, etc.) is concentrated in accommodations that have no recent reviews (`number_of_reviews_ltm == 0`).\n",
    "\n",
    "**Rationale:** These accommodations can be considered \"inactive\" or potentially new, and it is plausible that they do not have enough recent data to populate fields related to ratings and review history. By segmenting the dataset based on this criterion, a more targeted cleaning and analysis strategy can be developed for each group.\n",
    "\n",
    "**Methodology:** To validate this hypothesis, the dataset will be divided into two distinct subsets:\n",
    "1. **Active Accommodations:** Accommodations with one or more reviews (`number_of_reviews_ltm > 0`).\n",
    "\n",
    "2. **Inactive/New Listings:** Listings with zero reviews (`number_of_reviews_ltm == 0`).\n",
    "\n",
    "After splitting, a `null` value analysis will be performed on each subset. This will allow a direct comparison of data quality and confirm whether `nulls` are, in fact, disproportionately present in the \"Inactive/New\" group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3228deec-1c9e-40c1-a700-93b3d00ea9f6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Dataframes division"
    }
   },
   "outputs": [],
   "source": [
    "# df_review: rooms WITH reviews in the last 12 months\n",
    "df_reviews = df_bnb.filter(f.col(\"number_of_reviews_ltm\") > 0)\n",
    "\n",
    "# df_without_review: rooms WITHOUT reviews in the last 12 months \n",
    "df_without_reviews = df_bnb.filter(f.col(\"number_of_reviews_ltm\") == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b4d8a73-6835-42b4-aa54-fdacd2215b0a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data consistency and null behavior"
    }
   },
   "outputs": [],
   "source": [
    "# original dataframe row count\n",
    "total_original = df_bnb.count()\n",
    "\n",
    "# new dataframes row count\n",
    "with_reviews_count = df_reviews.count()\n",
    "without_reviews_count = df_without_reviews.count()\n",
    "\n",
    "# data consistency verification\n",
    "if total_original == (with_reviews_count + without_reviews_count):\n",
    "    print(\"Verification is successful!.\")\n",
    "else:\n",
    "    print(\"The sum of the parts is NOT equal to the total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc5ce92d-b173-4b29-86a1-3f3379d27d30",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyzing null behavior: df_reviews"
    }
   },
   "outputs": [],
   "source": [
    "# calculating null counts and percentages for each column\n",
    "mappingNullvalues(df_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb508302-c92b-4353-9d7a-d17be5bfec9d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyzing null behavior: df_without_reviews"
    }
   },
   "outputs": [],
   "source": [
    "# calculating null counts and percentages for each column\n",
    "mappingNullvalues(df_without_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c968028-146b-42b9-8c77-e10caa3d3dcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Conclusion & New Hypothesis**\n",
    "\n",
    "The analysis confirms our initial hypothesis. The null rate for review-related columns is higher in the inactive group.\n",
    "\n",
    "An important secondary finding is that the null rate for `price` also dropped significantly, from **~40%** to **12%** in the active listings group. This suggests a strong association between recent activity and the commercial viability of a listing.\n",
    "\n",
    "However, limiting our analysis to only the 11,059 listings with reviews in the last 12 months appears to be too restrictive. We observed that approximately 12,000 listings with valid price information are excluded by this filter, suggesting they could still be relevant for our revenue analysis despite not having a review in the past year. \n",
    "\n",
    "We hypothesize that expanding the timeframe to include a longer period of \"recent activity\" could provide a larger, still relevant, dataset. To test this, we will now analyze listings with a last_review date within the last 2.5 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79fe105f-db7e-4ffa-8253-84bfc94198cf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Amplifying last_review time range"
    }
   },
   "outputs": [],
   "source": [
    "# df_review: rooms WITH at least 1 review in the last 30 months (considering august-2025)\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# getting the most recent last_review date \n",
    "''' We ar doing this to create a dashboard that do not require manual updates\n",
    "setting a relative time helps us understing the most recente listings behavior\n",
    "and also give us flexibility to change it if necessary'''\n",
    "max_last_review = df_bnb.agg(f.max(\"last_review\")).first()[0]\n",
    "\n",
    "# setting range to 2.5-year (30 months)\n",
    "rev_range = 30\n",
    "\n",
    "# calculating the lower bound date (max_last_review - time range)\n",
    "lower_bound = f.add_months(f.lit(max_last_review), - rev_range)\n",
    "\n",
    "# filtering rows where last_review is not null and is greater than lower_bound\n",
    "df_reviews_ranged = df_bnb.filter(\n",
    "                 (f.col(\"last_review\").isNotNull()) &\n",
    "                 (f.col(\"last_review\") >= lower_bound)\n",
    "            )\n",
    "\n",
    "# calculating null counts and percentages for each column\n",
    "mappingNullvalues(df_reviews_ranged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1e6883b-a09a-46d5-b8a6-f47953f6ca81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Conclusion & Next Hypothesis\n",
    "\n",
    "The analysis of the 2.5-year window reveals a clear trade-off:\n",
    "-   **12-Month Window:** Higher data quality (only 12% null prices) but a smaller dataset (~11k rows).\n",
    "-   **30-Month Window:** A significantly larger dataset (~16k rows) but with a slight decrease in quality (18% null prices).\n",
    "\n",
    "This confirms our suspicion that older records are more likely to have missing price information. The presence of a `price` appears to be a crucial indicator of a listing's overall data quality and commercial viability.\n",
    "\n",
    "This leads to our next core hypothesis: that the dataset can be effectively segmented into \"high-quality\" and \"low-quality\" records based solely on the presence of a `price`.\n",
    "\n",
    "**Next Step:** To test this, the next analysis will compare the overall data completeness of two groups:\n",
    "1.  Listings where `price` is present.\n",
    "2.  Listings where `price` is null.\n",
    "\n",
    "The results of this comparison will inform the final strategic decision on which records to include in our `df_final` dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "693ecfa7-d703-4e3e-829d-eddf88fa4e8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2.2 Price-Based Analysis\n",
    "\n",
    "**Hypothesis:** Accommodations where `price` is not `null` are expected to represent higher-quality records and, therefore, _have fewer null values_ in other columns when compared to accommodations where `price` is missing.\n",
    "\n",
    "**Rationale:** The absence of a price may indicate an incomplete, outdated, or invalid record. It is plausible that such records are also missing other important information, impacting overall data quality.\n",
    "\n",
    "**Methodology:** To test this hypothesis, the dataframe will be segmented into two groups:\n",
    "1. price is not null\n",
    "2. price is null \n",
    "\n",
    "A `null` value analysis will be performed on each group to compare the overall data completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "851c9f48-c810-441d-85d1-98886d80065e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Separating views based on having a price"
    }
   },
   "outputs": [],
   "source": [
    "# df_with_price: Accommodations WITH price\n",
    "df_with_price = df_bnb.filter(f.col(\"price\").isNotNull())\n",
    "\n",
    "# df_without_price: Accommodations WITHOUT price\n",
    "df_without_price = df_bnb.filter(f.col(\"price\").isNull())\n",
    "\n",
    "# original dataframe row count\n",
    "total_original = df_bnb.count()\n",
    "\n",
    "# new dataframes row count\n",
    "with_price_count = df_with_price.count()\n",
    "without_price_count = df_without_price.count()\n",
    "\n",
    "# checking data consistency\n",
    "if total_original == (with_price_count + without_price_count):\n",
    "    print(\"verification was successful!\")\n",
    "else:\n",
    "    print(\"The sum of the parts is NOT equal to the total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a9805ce-c9da-4d9b-9040-761485bd244d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyzing null behavior: df_with_price"
    }
   },
   "outputs": [],
   "source": [
    "# calculating null counts and percentages for each column\n",
    "mappingNullvalues(df_with_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48c0111a-d214-453d-9a36-fdc5f2873d28",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyzing null behavior: df_without_price"
    }
   },
   "outputs": [],
   "source": [
    "# calculating null counts and percentages for each column\n",
    "mappingNullvalues(df_without_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe3f9d99-d967-4cc4-8f6f-e67eba237815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Conclusion: Price as a Key Indicator of Data Quality\n",
    "\n",
    "The analysis strongly confirms our hypothesis. The group of listings **with a price** (`df_with_price`) demonstrates significantly higher data quality across nearly all columns compared to the group without a price. For instance, the null rate for `bathrooms` drops from **98.19%** to **0.03%**, and for `host_response_rate` from **76.79%** to **13.71%**.\n",
    "\n",
    "This investigation, combined with the previous analysis on review activity, has allowed us to identify the two primary characteristics of a high-quality, relevant listing for our analysis:\n",
    "\n",
    "1.  **Commercial Viability:** The listing must have a `price`. The absence of a price has proven to be the strongest indicator of an incomplete or inactive record.\n",
    "2.  **Recent Market Activity:** The listing should have recent engagement. As determined previously, filtering by recent reviews (e.g., within the last 12 or 30 months) is an effective proxy metric for this.\n",
    "\n",
    "Therefore, relying on just one of these filters is suboptimal. The most robust strategy is to define our final analytical dataset, `df_final`, by selecting the records that satisfy **both** criteria. This will create a core dataset of listings that are both commercially viable and recently active, providing the most reliable foundation for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed34b49f-0dfc-4690-95eb-1f705976b711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.3 Defining the Final Analysis Dataset: `df_final`\n",
    "\n",
    "The preceding analyses have established two primary criteria for identifying relevant listings: **Commercial Viability** (the presence of a `price`) and **Recent Market Activity** (a review within the last 2.5 years).\n",
    "\n",
    "The strategic decision has been made to create a broader and more inclusive dataset for our analysis. This approach ensures that we do not discard any record that meets at least one of these relevance criteria.\n",
    "\n",
    "This comprehensive dataset is ideal for our objectives of exploratory analysis, creating comprehensive dashboards, and understanding the market as a whole. It allows us to analyze the full spectrum of listings that are either commercially viable, recently active, or both.\n",
    "\n",
    "Therefore, `df_final` will be constructed as the **union** of these two subsets:\n",
    "1.  All listings that have a valid `price`.\n",
    "2.  All listings that have a `last_review` within the last 2.5 years.\n",
    "\n",
    "The resulting DataFrame will serve as the complete and final basis for our subsequent cleaning and normalization steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5029f306-d7a9-423e-a15b-bf5f3cb17d0d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating df_final"
    }
   },
   "outputs": [],
   "source": [
    "# inner join df and dfplus_no_dups\n",
    "df_final = df_reviews_ranged.union(df_with_price).distinct()\n",
    "\n",
    "# checking data consistency\n",
    "final_count = df_final.count()\n",
    "id_final_count = df_final.select(\"id\").distinct().count()\n",
    "\n",
    "if final_count == id_final_count:\n",
    "    print(\"dataframes union was successful!\")\n",
    "else:\n",
    "    print(\"dataframe ids are not unique, union unsuccessful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce1da95e-9ac4-428a-b6e4-fc851a77b09d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Null values mapping"
    }
   },
   "outputs": [],
   "source": [
    "# generating null counts and percentages for each column\n",
    "mappingNullvalues(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d923289-8434-43f2-b14a-b1d0236d41cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "reflex√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2d2ef02-c43b-404c-91ea-05976c605b39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 2.4 Removing unecessary colunms\n",
    "\n",
    "Based on our analytical goal (understanding revenue drivers), the following columns have been identified as unnecessary. The primary criterion for removal is a lack of a clear and relevant relationship to successful ad behavior.\n",
    "\n",
    "**Action Plan:** Instead of actively dropping these columns from the `df_final` DataFrame now, they will simply be **excluded** during the database normalization step. This is a more efficient approach, as we will only select the columns we need when creating the new tables\n",
    "\n",
    "colunm  | motivation\n",
    "--- |---\n",
    "neighborhood_highlights | when neighborhood_overview has data, neighborhood_highlights = true otherwise it's false. we already have this info at neighborhood_overview column\n",
    " has_avaliability | data dictionary doesn't explain the column behavior in a way that makes sense with avaliability_365\n",
    " host_name | doesn't impact in host appealing for the clients\n",
    " calculated_host_listings_count data | dictionary doesn't explain how it's calculated\n",
    " host_total_listings_count | dictionary doesn't explain how it's calculated and we can count the listings\n",
    "host_listings_count| dictionary doesn't explain how it's calculated and we can count the listings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4287b20-20a6-45ad-81a3-4a14693cfca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Database normalization\n",
    "\n",
    "![colunms from both dataframe](/Volumes/workspace/default/primeirovolume/colunms_analysis.jpg)\n",
    "\n",
    "\n",
    "The columns in our dataset can be grouped into four primary categories: **Host, Room, Review** and **Location** data. To improve data integrity, flexibility and storage efficiency, we will normalize this dataset into three primary tables. \n",
    "\n",
    "Additionally, the `amenities` column contains a list of features for each listing. To properly structure this one-to-may relationship, a fourth table, `df_amenities`, will be crated. \n",
    "\n",
    "df | type of data | Primary Key (PK) | Foreign Keys (FK)\n",
    "---- | ----- | -----  | ----- \n",
    "df_rooms | room details | `id` | `host_id`(references `df_hosts`)\n",
    "df_hosts | host related information | `host_id` | -\n",
    "df_reviews | client perception related information | `room_id` | `room_id`(references `df_rooms`)\n",
    "df_amenities | room amenities list |  `room_id`  | `room_id`(references `df_rooms`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f82f281-ddd5-4b79-b0f3-ab6e1640e593",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760715712949}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1760983792280}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": "df_rooms"
    }
   },
   "outputs": [],
   "source": [
    "# creating table df_rooms\n",
    "df_rooms = df_final.select(\n",
    "            \"id\", \"name\",\"host_id\", \"neighbourhood_group\", \"neighbourhood\", \"latitude\", \"longitude\",\n",
    "            \"room_type\", \"price\", \"minimum_nights\", \"maximum_nights\", \"availability_365\",\n",
    "            \"neighborhood_overview\", \"license\", \"accommodates\",\n",
    "            \"bathrooms\", \"bedrooms\", \"estimated_occupancy_l365d\"\n",
    ")\n",
    "\n",
    "df_rooms.describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecbeb7a8-896e-42d2-9c09-7a5e0bfd4035",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760976585501}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "df_hosts"
    }
   },
   "outputs": [],
   "source": [
    "# creating table df_hosts and removing duplicates at the same time\n",
    "df_hosts = df_final.select(\n",
    "           \"host_id\", \"host_since\", \"host_response_time\", \"host_response_rate\",\n",
    "            \"host_acceptance_rate\", \"host_is_superhost\", \"host_identity_verified\"\n",
    "            ).dropDuplicates([\"host_id\"])\n",
    "\n",
    "df_hosts.describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58049ab1-92b5-47e1-aa5a-1eba0521ccae",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760976585541}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "df_reviews"
    }
   },
   "outputs": [],
   "source": [
    "# creating table df_reviews\n",
    "df_reviews = df_final.select(\"id\", \"number_of_reviews\",\"last_review\", \"reviews_per_month\",\n",
    "            \"number_of_reviews_ltm\", \"number_of_reviews_ly\",\"first_review\", \"review_scores_rating\",\n",
    "            \"review_scores_accuracy\", \"review_scores_cleanliness\", \"review_scores_checkin\",\n",
    "            \"review_scores_communication\", \"review_scores_location\", \"review_scores_value\"\n",
    "            ).withColumnRenamed(\"id\", \"room_id\")\n",
    "\n",
    "df_reviews.describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bffbd7be-bb70-4399-a4cd-755d5e19120a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "df_amenities"
    }
   },
   "outputs": [],
   "source": [
    "# creating table df_amenities\n",
    "df_amenities = df_final.select(\"id\",\"amenities\"\n",
    "              ).withColumnRenamed(\"id\", \"room_id\")\n",
    "\n",
    "df_amenities.describe().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc804ff4-d324-4426-8d1f-41ef956c0d5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Understanding and cleaning data\n",
    "\n",
    "1. Casting types\n",
    "2. Treating null/empty fields\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c28caa4-4b77-47a8-84c6-e19d0244f49c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.1 df_rooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67fb2e05-40b4-48ab-8f9b-4dacb5affad2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 4.1.1 Understanding columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27f65327-fa1a-4ae7-80c4-0e45f5aaf2bc",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760982124043}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      },
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760982125673}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1760983896255}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 1
      }
     },
     "title": "Show df_room data"
    }
   },
   "outputs": [],
   "source": [
    "# schema \n",
    "# checking the data types to plan for the casting step\n",
    "df_rooms.printSchema()\n",
    "\n",
    "# displaying descriptive statistics\n",
    "df_rooms.describe().display()\n",
    "df_rooms.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a6865fa-c6d2-4f4d-96eb-224c0b628e70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Regarding data types:**\n",
    "\n",
    "The `bathrooms`column could not be directly cast to an integer because it contains float values (e.g., \"1.5\"). This requires special handling, as a listing can have a \"half-bath\". The other columns have the expected types based on the data dictionary.\n",
    "\n",
    "**Observation and Next Steps Regarding Price Distribution:**\n",
    "\n",
    "The descriptive statistics reveal an extremely wide range for the `price` column, with a minimum value likely near $3 and a maximum value exceeding $50,000. This discrepancy for a daily rate strongly suggests the presence of outliers or potential data entry errors.\n",
    "\n",
    "Such extreme values can significantly distort measures like the mean and median, potentially leading to inaccurate conclusions about typical pricing.\n",
    "\n",
    "These price outliers will be identified and handled using the robust Interquartile Range (IQR) method. This will provide a cleaner dataset focused on more representative pricing patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00da3d58-5366-4a91-8603-749008398bab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 4.1.2 Type casting\n",
    "\n",
    "First, we address the specific case of the `bathrooms` column, which requires special handling due to the presence of \"half-bath\" values.\n",
    "\n",
    "Next, we cast the geographical coordinates to ensure they are in the correct numeric format for mapping visuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2603e064-9a6a-44ed-a763-0afb382450af",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1761082135092}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "bathroom type casting"
    }
   },
   "outputs": [],
   "source": [
    "### BATHROOMS\n",
    "''' \n",
    "From date dictonary information, it should be numeric representing the amount of bathrooms in the property.\n",
    "However, there are values we could classify as [float]. After researching on Airbnb website, we found that the value\n",
    "\"0.5\" could represent a bathroom without a bathing facility or wihout a toilet. With that information, we will create\n",
    "a complementar colunm called \"incomplete_bathroom\" do register that information in a way we can analize with more clarity.\n",
    "'''\n",
    "# counting incomplete bathrooms (if the row has \".5\" count it)\n",
    "inc_bath_count = df_rooms.filter(df_rooms.bathrooms.rlike(\"\\\\.5$\")).count()\n",
    "print(f\"ther are {inc_bath_count} incomplete bathrooms\")\n",
    "\n",
    "# removing 0.5 from bathrooms conlumn and keeping the null values\n",
    "df_rooms = df_rooms.withColumn(\n",
    "            # creating a column to register if the value is 0.5 keeping null values\n",
    "            \"incomplete_bathroom\",\n",
    "            f.when(\n",
    "                f.col(\"bathrooms\").isNotNull() & \n",
    "                f.col(\"bathrooms\").rlike(\"\\\\.5$\"),\n",
    "                1\n",
    "            ).otherwise(0)\n",
    "        ).withColumn(\"bathrooms\",\n",
    "            # removing 0.5 from bathrooms conlumn and keeping the null values and casting it as int\n",
    "            f.when(\n",
    "                  f.col(\"bathrooms\").isNotNull(),\n",
    "                  f.split(f.col(\"bathrooms\"), \"\\\\.\").getItem(0).try_cast(\"int\"))\n",
    "            .otherwise(None)\n",
    "        )\n",
    "\n",
    "# verifying data consistency\n",
    "# if bathroom col has more then 0 values ending with \".5\", fail, otherwise: success\n",
    "if df_rooms.filter(f.col(\"bathrooms\").rlike(\"\\\\.5$\")).count() > 0:\n",
    "   print(\"- conversion failed\")\n",
    "else:\n",
    "   print(\"- conversion was successful\")\n",
    "\n",
    "# getting the rows count from the new column by summing the values\n",
    "inc_bath_count2 = df_rooms.agg(f.sum(\"incomplete_bathroom\")).collect()[0][0]\n",
    "\n",
    "# comparing the two counts\n",
    "if inc_bath_count2 != inc_bath_count:\n",
    "   print(\"- incomplete_bathroom column creation failed.\")\n",
    "else:\n",
    "   print(\"- incomplete_bathroom column creation was successful!\")    \n",
    "df_rooms.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ef092e8-448f-4a86-9e0b-eaa089de85f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Latitude and Longitude"
    }
   },
   "outputs": [],
   "source": [
    "# latitude and longitude\n",
    "\n",
    "# cast latitude and longitude from string to double\n",
    "df_rooms = df_rooms.withColumn(\n",
    "    \"latitude\",\n",
    "    f.col(\"latitude\").cast(\"double\")\n",
    ").withColumn(\n",
    "    \"longitude\",\n",
    "    f.col(\"longitude\").cast(\"double\")\n",
    ")\n",
    "\n",
    "print(\"\\nCasting for latitude and longitude complete.\")\n",
    "\n",
    "# --- Verification for Lat/Lon ---\n",
    "print(\"\\nSchema after casting latitude and longitude:\")\n",
    "df_rooms.printSchema()\n",
    "\n",
    "print(\"\\nSample values:\")\n",
    "df_rooms.select(\"latitude\", \"longitude\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f757b5-7f30-445b-bec4-9a33b68c575f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyzing null behavior"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the count and percentage of null values in each column\n",
    "mappingNullvalues(df_rooms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "186a4e07-5240-4d84-a66c-c35691efc231",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 4.1.3 Removing Price Outliers (IQR Method)\n",
    "\n",
    "To ensure our revenue analysis is based on plausible and representative data, we will remove price outliers using the Interquartile Range (IQR) method.\n",
    "\n",
    "The IQR method identifies outliers as values falling below Q1 - 1.5*IQR or above Q3 + 1.5*IQR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "378ce548-6c51-4bb1-a5ee-cb09ebca0376",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Outlier detection"
    }
   },
   "outputs": [],
   "source": [
    "df_rooms.agg(f.min(\"price\"), f.max(\"price\")).show()\n",
    "\n",
    "# --- 1. IQR Calculation ---\n",
    "# calculate Q1 (25th percentile) and Q3 (75th percentile) for the 'price' \n",
    "quantiles = df_rooms.approxQuantile(\"price\", [0.25, 0.75], 0.0)\n",
    "\n",
    "# proceed only if quantiles were successfully calculated\n",
    "if len(quantiles) == 2 :\n",
    "    Q1 = quantiles[0]\n",
    "    Q3 = quantiles[1]\n",
    "    print(f\"\\nQ1 Calculated: {Q1}\")\n",
    "    print(f\"Q3 Calculated: {Q3}\")\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    print(f\"IQR: {IQR}\")\n",
    "    print(f\"Calculated Lower Bound: {lower_bound}\")\n",
    "    print(f\"Calculated Upper Bound: {upper_bound}\")\n",
    "\n",
    "    # --- 2. Filtering ---\n",
    "    # filter the DataFrame, keeping rows within the IQR bounds OR where price is null.\n",
    "    df_rooms_no_outliers = df_rooms.filter(\n",
    "        (f.col(\"price\") >= lower_bound) &\n",
    "        (f.col(\"price\") <= upper_bound) |  \n",
    "        f.col(\"price\").isNull()             \n",
    "    )\n",
    "\n",
    "    count_after = df_rooms_no_outliers.count()\n",
    "    print(f\"\\nRecord count after price outlier removal: {count_after}\")\n",
    "    print(f\"Number of price outliers removed: {df_rooms.count() - count_after}\")\n",
    "\n",
    "    # --- 3. Verification ---\n",
    "    print(\"\\nPrice range after outlier removal:\")\n",
    "    df_rooms_no_outliers.agg(f.min(\"price\"), f.max(\"price\")).show()\n",
    "\n",
    "    # overwriting the original df_rooms with the cleaned version\n",
    "    df_rooms = df_rooms_no_outliers\n",
    "    print(\"\\ndf_rooms updated with outliers removed.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nWARNING: Could not calculate Q1/Q3 for 'price'. Outlier removal was skipped. df_rooms remains unchanged.\")\n",
    "\n",
    "# displaying a sample or schema to confirm\n",
    "df_rooms.select(\"price\").summary(\"min\", \"max\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f680647e-e613-4f7e-bbfb-d131e576a4c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# showing listings with price = $3\n",
    "print(f\"Listings with price = ${3}:\")\n",
    "df_low_price = df_rooms.filter(f.col(\"price\") == 3)\n",
    "\n",
    "df_low_price.select(\n",
    "    \"id\", \"name\", \"host_id\", \"neighbourhood\", \"room_type\",\n",
    "    \"price\", \"minimum_nights\", \"accommodates\"\n",
    ").show(50, truncate=False) \n",
    "\n",
    "print(f\"Total count of listings with price <= ${3}: {df_low_price.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c613d77-4ad7-4a10-b7ec-4cafb7f7bce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "To ensure the integrity and realism of our price analysis, this single anomalous record will be **removed** from the `df_rooms` dataset.\n",
    "'''\n",
    "# removing the anomalous record (if price is not 3)\n",
    "df_rooms = df_rooms.filter(\n",
    "                    (f.col(\"price\")!= 3)|     \n",
    "                    f.col(\"price\").isNull()    \n",
    ")\n",
    "\n",
    "# displaying a sample or schema to confirm\n",
    "df_rooms.select(\"price\").summary(\"min\", \"max\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92773fc3-22ee-4c20-9b13-c1b95efe0a6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# getting number of records in df_rooms\n",
    "total_records = df_rooms.count()\n",
    "\n",
    "# getting the number of null values ‚Äã‚Äãin the 'price' column.\n",
    "null_prices = df_rooms.filter(f.col(\"price\").isNull()).count()\n",
    "\n",
    "print(f\"Records count in df_rooms: {total_records}\")\n",
    "print(f\"Null values ‚Äãcount ‚Äãin 'price': {null_prices}\")\n",
    "\n",
    "# showing null values percentage ‚Äã‚Äãin 'price'\n",
    "if total_records > 0:\n",
    "    percentual_null_price = (null_prices / total_records) * 100\n",
    "    print(f\"Percentage of null values ‚Äã‚Äãin the 'price' column: {percentual_null_price:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f1279b-a042-4868-872b-f16de37b580d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####4.1.4 Synchronizing Related Tables\n",
    "\n",
    "Now that `df_rooms` has been filtered (e.g., price outliers removed), it represents our definitive set of valid listings for the analysis. However, the `df_hosts`, `df_reviews`, and `df_amenities` DataFrames were created earlier from a potentially larger dataset and may still contain records related to listings that are no longer present in the cleaned `df_rooms`.\n",
    "\n",
    "To ensure consistency across our normalized tables, we will now filter `df_hosts`, `df_reviews`, and `df_amenities` to keep only the records that correspond to the valid `id`s and `host_id`s remaining in the cleaned `df_rooms`.\n",
    "\n",
    "**Methodology:**\n",
    "1.  Extract the distinct `id`s (room IDs) from the current `df_rooms`.\n",
    "2.  Extract the distinct `host_id`s from the current `df_rooms`.\n",
    "3.  Perform a **semi join** between `df_reviews` / `df_amenities` and the valid room IDs. A semi join keeps only the rows from the left DataFrame (`df_reviews`/`df_amenities`) that have a matching key in the right DataFrame (valid room IDs).\n",
    "4.  Perform a semi join between `df_hosts` and the valid host IDs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f249a2-447e-42b0-a2bf-732c254e5307",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Synchronizing dataframes"
    }
   },
   "outputs": [],
   "source": [
    "# --- Re-synchronize tables based on the FINAL df_rooms ---\n",
    "print(\"Re-synchronizing related tables with the final df_rooms...\")\n",
    "\n",
    "# getting list of valid room IDs present in the final df_rooms\n",
    "# Renomeamos 'id' para 'room_id' para facilitar o join com df_reviews/df_amenities\n",
    "valid_room_ids = df_rooms.select(f.col(\"id\").alias(\"room_id\")).distinct()\n",
    "print(f\"Number of valid room IDs found in final df_rooms: {valid_room_ids.count()}\")\n",
    "\n",
    "# getting list of valid host IDs present in the final df_rooms\n",
    "valid_host_ids = df_rooms.select(\"host_id\").distinct()\n",
    "print(f\"Number of valid host IDs found in final df_rooms: {valid_host_ids.count()}\")\n",
    "\n",
    "# filtering df_reviews to keep only reviews for valid rooms\n",
    "count_reviews_before = df_reviews.count()\n",
    "\n",
    "# semi join': it keeps the rows from df_reviews that have a match in valid_room_ids\n",
    "df_reviews = df_reviews.join(valid_room_ids, on=\"room_id\", how=\"semi\")\n",
    "count_reviews_after = df_reviews.count()\n",
    "print(f\"df_reviews synced: {count_reviews_before} -> {count_reviews_after} rows\")\n",
    "\n",
    "# filteringdf_hosts to keep only hosts present in the final df_rooms\n",
    "count_hosts_before = df_hosts.count()\n",
    "df_hosts = df_hosts.join(valid_host_ids, on=\"host_id\", how=\"semi\")\n",
    "count_hosts_after = df_hosts.count()\n",
    "print(f\"df_hosts synced: {count_hosts_before} -> {count_hosts_after} rows\")\n",
    "\n",
    "# filtering df_amenities (assuming it also uses 'room_id')\n",
    "count_amenities_before = df_amenities.count()\n",
    "df_amenities = df_amenities.join(valid_room_ids, on=\"room_id\", how=\"semi\")\n",
    "count_amenities_after = df_amenities.count()\n",
    "print(f\"df_amenities synced: {count_amenities_before} -> {count_amenities_after} rows\")\n",
    "\n",
    "print(\"\\nSynchronization complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fff17275-8352-4383-bee8-01033af50220",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 4.1.5 Treating Null/Empty Fields\n",
    "\n",
    "The null value analysis indicates that `neighborhood_overview`, `price`, `bathrooms`, and `bedrooms` are the primary columns with missing data in the `df_rooms` table. A differentiated strategy will be applied to handle these nulls.\n",
    "\n",
    "**Imputation Strategy:**\n",
    "\n",
    "-   **`price`**: **No imputation will be performed.** We hypothesize that null prices are linked to inactive listings. Therefore, the strategy will be to **filter the dataset to include only records with a valid price** for any subsequent revenue-focused analysis. This ensures that our financial calculations are based solely on commercially viable listings.\n",
    "\n",
    "-   **`bathrooms` and `bedrooms`**: The nulls in these columns will be left as is for now. They can be addressed in a more detailed analysis later if required. \n",
    "\n",
    "-   **`neighborhood_overview`**: This is the only column where imputation will be performed at this time. Given its potential for contextual insights about neighborhood, nulls will be replaced with `\"Not Informed\"`.\n",
    "\n",
    "**Action Plan:**\n",
    "Based on the strategy above, the only action to be executed now is the imputation of the `neighborhood_overview` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7a86097-b9a5-440d-a262-c4aff91a2cc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# treating the 'neighborhood_overview' column\n",
    "df_rooms = df_rooms.na.fill(\n",
    "    value=\"Not Informed\",\n",
    "    subset=[\"neighborhood_overview\"]\n",
    ")\n",
    "\n",
    "# verification\n",
    "after_count = df_rooms.filter(f.col('neighborhood_overview').isNull()).count()\n",
    "print(f\"Null count for 'neighborhood_overview' after imputation: {after_count}\")\n",
    "if after_count == 0:\n",
    "    print(\"‚úÖ Verification successful.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e8a018e-368c-421c-ba4d-f3ac4ed4e7dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.2 df_hosts\n",
    "##### 4.2.1 Understanding columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe4557c-0341-4779-8cb0-a313e5bbc732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# schema \n",
    "# checking the data types to plan for the casting step\n",
    "df_hosts.printSchema()\n",
    "\n",
    "# checking  for duplicate host_ids\n",
    "# This is a sanity check to confirm that the .dropDuplicates() during creation was successful. The expected output is 0.\n",
    "duplicate_count = df_hosts.count() - df_hosts.select(\"host_id\").distinct().count()\n",
    "print(f\"Number of duplicate host_ids: {duplicate_count}\")\n",
    "\n",
    "# displaying descriptive statistics\n",
    "df_hosts.describe().display()\n",
    "\n",
    "df_hosts.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fab0a3de-be5c-4f63-9204-ce48faf9be73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Schema Analysis & Casting Plan**\n",
    "\n",
    "The `printSchema()` output and the uniqueness check confirm that the `df_hosts` DataFrame is correctly structured with a unique `host_id` for each record. The `host_id` (string) and `host_since` (date) columns already have appropriate data types.\n",
    "\n",
    "However, the analysis reveals that several other columns representing numerical or boolean data are currently stored as strings. To enable accurate calculations and logical filtering, the following transformations will be performed in the next step, **\"4.2.2 Casting Types\"**:\n",
    "\n",
    "-   **`host_response_rate`** and **`host_acceptance_rate`**: These will be converted from string percentages (e.g., \"95%\") to a numeric `double` type (e.g., 0.95).\n",
    "-   **`host_is_superhost`** and **`host_identity_verified`**: These will be converted from string flags (e.g., \"t\", \"f\") to a `boolean` type (True/False).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61ebc8b6-1643-454e-9237-69d458b52515",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyzing null behavior"
    }
   },
   "outputs": [],
   "source": [
    "# calculating the count and percentage of null values in each column\n",
    "mappingNullvalues(df_hosts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36e6ae86-ee9a-4708-a147-1a03ce4ba463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Decision**\n",
    "\n",
    "Based on the analysis above, all columns in the `df_hosts` table have a very low percentage of null values. Therefore, all columns are considered valuable and **no columns will be removed**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5857a077-7a89-4aa8-8fd4-910f452d7f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####4.2.2 Casting types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20a65d2f-09a5-4e43-a65b-9e1e5fc51f6a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Casting types for df_hosts"
    }
   },
   "outputs": [],
   "source": [
    "# converting \"host_response_rate\" from string \"100%\" to the number 1.0\n",
    "df_hosts = df_hosts.withColumn(\n",
    "    \"host_response_rate\",\n",
    "    f.regexp_replace(\n",
    "        f.col(\"host_response_rate\"),\n",
    "        \"%\",\n",
    "        \"\"\n",
    "    ).cast(\"double\")/100   \n",
    ")\n",
    "\n",
    "# converting \"host_acceptance_rate\" from string \"100%\" to the number 1.0\n",
    "df_hosts = df_hosts.withColumn(\n",
    "    \"host_acceptance_rate\",\n",
    "     f.regexp_replace(\n",
    "            f.col(\"host_acceptance_rate\"),\n",
    "            \"%\",\n",
    "            \"\"\n",
    "        ).cast(\"double\")/100\n",
    ")\n",
    "\n",
    "# converting \"host_is_superhost\" from string \"t\" or \"f\" to boolean True or False\n",
    "df_hosts = df_hosts.withColumn(\n",
    "    \"host_is_superhost\",\n",
    "    f.when(f.col(\"host_is_superhost\") == \"t\", True)\n",
    "     .when(f.col(\"host_is_superhost\") == \"f\", False)\n",
    "     .otherwise(None)\n",
    ")\n",
    "\n",
    "# converting \"host_identity_verified\" from string \"t\" or \"f\" to boolean True or False\n",
    "df_hosts = df_hosts.withColumn(\n",
    "    \"host_identity_verified\",\n",
    "    f.when(f.col(\"host_identity_verified\") == \"t\", True)\n",
    "     .when(f.col(\"host_identity_verified\") == \"f\", False)\n",
    "     .otherwise(None)\n",
    ")\n",
    "\n",
    "display(df_hosts)\n",
    "df_hosts.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0776ddd8-c477-4979-b7c7-0a0bb564b654",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Statistics"
    }
   },
   "outputs": [],
   "source": [
    "# data statistics\n",
    "display(\n",
    "        df_hosts.select(\"host_response_rate\", \"host_acceptance_rate\")\n",
    "        .summary(\"count\", \"mean\", \"stddev\", \"min\", \"50%\", \"max\").show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11aac180-4860-461a-943b-af25ccf2073c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Insight: Analysis of Host Response and Acceptance Rates**\n",
    "\n",
    "The summary statistics for `host_response_rate` and `host_acceptance_rate` reveal a significant difference between the mean (average) and the median (50th percentile), which points to a skewed distribution of the data.\n",
    "\n",
    "**Key Observations:**\n",
    "-   For `host_response_rate`, the **median is 1.0 (100%)**, while the **mean is significantly lower at approximately 0.87 (87%)**.\n",
    "-   A similar pattern is observed for `host_acceptance_rate`, with a **median of 0.94 (94%)** and a lower **mean of approximately 0.76 (76%)**.\n",
    "\n",
    "**Interpretation:**\n",
    "This discrepancy confirms that the data for both metrics is **negatively skewed (left-skewed)**. A large number of hosts are highly responsive, with at least 50% of them having a perfect 100% response rate. The lower mean is being pulled down by a subset of hosts with very low response and acceptance rates. These outliers have a disproportionate effect on the average.\n",
    "\n",
    "**Business Implication & Next Step:**\n",
    "This suggests there may be two distinct segments of host behavior: a majority of highly engaged hosts and a \"long tail\" of less responsive ones. The latter could represent an opportunity for engagement or a potential risk to the guest experience.\n",
    "\n",
    "To better visualize this distribution and clearly illustrate the difference between the mean and median, creating a **histogram** or a **box plot** for both columns would be an excellent next step. This would provide a powerful visual insight into host performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5af10e23-2f2a-4d16-9b6f-9adec9e9d808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####4.2.3 Treating null/empty fields\n",
    "\n",
    "After the type casting, the next step is to handle the remaining null values in the `df_hosts` table.\n",
    "\n",
    "**Strategy: Imputation over Deletion**\n",
    "\n",
    "The primary goal is to treat all null values **without deleting any rows**. This is a critical decision based on the following reason:\n",
    "\n",
    "-   **Referential Integrity:** The `df_hosts` DataFrame is a supplementary table to `df_rooms`. Deleting host records from this table would create rooms without an associated host, which would break the referential integrity of our final dataset and lead to data loss in the dashboard.\n",
    "\n",
    "**Action Plan & Implementation**\n",
    "\n",
    "Based on the nature of each column, the following imputation strategy will be applied:\n",
    "\n",
    "-   **Categorical Columns (`host_response_time`):** Null values will be replaced with the string `\"Not Informed\"`. This preserves the record while clearly marking the data as missing.\n",
    "\n",
    "-   **Numerical Rate Columns (`host_response_rate`, `host_acceptance_rate`):** Null values will be filled with `0`. This is a conservative choice reflecting a lack of recorded activity. We cannot assume an average value, as the host may be new or have insufficient data, and imputing a mean could falsely inflate their performance.\n",
    "\n",
    "-   **Boolean Columns (`host_is_superhost`, `host_identity_verified`):** Null values will be treated as `False`. This assumes that if a positive status (like being a Superhost) is not explicitly recorded as `True`, it does not apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ff3f4c-0f73-41a8-b36e-7f1befe8f73d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"Original data with nulls:\")\n",
    "# showing some rows that we know will be treated\n",
    "df_hosts.filter(f.col(\"host_response_rate\").isNull()).show(3, truncate=False)\n",
    "\n",
    "# applying the .na.fill() transformations and overwrite the df_hosts DataFrame\n",
    "df_hosts = df_hosts.na.fill(\n",
    "                    value=0,\n",
    "                    subset=[\"host_response_rate\", \"host_acceptance_rate\"]\n",
    "                ).na.fill(\n",
    "                    value=False,\n",
    "                    subset=[\"host_is_superhost\", \"host_identity_verified\"]\n",
    "                ).na.fill(\n",
    "                    value=\"Not Informed\",\n",
    "                    subset=[\"host_response_time\"]\n",
    "                )\n",
    "\n",
    "print(\"\\nData after imputation (showing the rows that were treated):\")\n",
    "\n",
    "# Verification: Filter by one of the imputed values to see the result\n",
    "df_hosts.filter(f.col(\"host_response_time\") == \"Not Informed\").show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a57c945e-cedc-4ff7-a57b-2df62c01f470",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyzing null behavior"
    }
   },
   "outputs": [],
   "source": [
    "# null mapping\n",
    "mappingNullvalues(df_hosts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb24687-294e-4ed8-8777-8b1b9717a732",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# data statistics\n",
    "df_hosts.select(\"host_response_rate\", \"host_acceptance_rate\").summary(\"count\", \"mean\", \"stddev\", \"min\", \"50%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8208e81-69aa-44f6-855b-6dd49e0dd28e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Impact of Null Imputation on Host Rate Metrics**\n",
    "\n",
    "\n",
    "After imputing `null` values with `0` for the `host_response_rate` and `host_acceptance_rate` columns, the summary statistics were recalculated.\n",
    "\n",
    "**Key Finding:**\n",
    "A significant drop was observed in the central tendency metrics. The median `host_acceptance_rate` fell from **0.94 to 0.67**, and the mean for both rates also decreased considerably.\n",
    "\n",
    "**Interpretation:**\n",
    "This confirms that the population of hosts with previously `null` rate information performs significantly differently than the population with recorded rates. By filling nulls with `0`, we have created a more complete and realistic picture of the entire host population, which now properly includes a substantial segment of hosts who, likely due to inactivity or insufficient interaction history, did not have a recorded performance rate. The previous, higher metrics were misleadingly optimistic as they only represented a subset of more active hosts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be3d3f82-40dc-4a8c-a9dc-ca2c9393b902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Important Considerations for Dashboard\n",
    "\n",
    "The imputation choices made here have a direct impact on statistical analysis and must be considered when building visualizations:\n",
    "\n",
    "-   **Impact on Averages:** Filling null rates with `0` will further **lower the mean (average)** for these metrics. When presenting these insights, it is crucial to prioritize the **median** as the primary measure of central tendency, as it is robust to these `0` values. The mean should only be displayed with a clear note explaining this imputation choice.\n",
    "\n",
    "-   **Impact on Proportions:** Similarly, treating nulls as `False` for boolean columns will increase the total count of \"Non-Superhosts\" and \"Unverified\" hosts. When displaying proportions (e.g., in pie charts or percentage), this context is essential to avoid misinterpreting the overall distribution of host types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf0fdb6f-fe3b-441b-b605-3750ed63cbd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.3 df_reviews\n",
    "#####4.2.1 Understanding columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a78b5ba-d617-4d1b-b9ae-4163c1dc8013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# schema \n",
    "# checking the data types to plan for the casting step\n",
    "df_reviews.printSchema()\n",
    "\n",
    "# checking for duplicate room_ids\n",
    "# sanity checking (expected output is 0)\n",
    "duplicate_count = df_reviews.count() - df_reviews.select(\"room_id\").distinct().count()\n",
    "print(f\"Number of duplicate room_ids: {duplicate_count}\")\n",
    "\n",
    "# displaying descriptive statistics\n",
    "df_reviews.describe().display()\n",
    "\n",
    "df_reviews.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a73bcae8-cadf-4872-8243-5d0637fadf4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Schema Analysis & Casting Plan**\n",
    "\n",
    "\n",
    "The `printSchema()` output and the uniqueness check confirm that the `df_reviews` DataFrame is correctly structured, with a unique `room_id` for each record.\n",
    "\n",
    "The analysis reveals that most columns already have the correct data types (`integer`, `date`, `double`). However, the `reviews_per_month` column, which should be numeric, is currently a `string`. The `.describe()` output also highlights a significant number of null values in the review-related columns (e.g., 24,259 total records vs. ~17,940 in `review_scores_rating`), which will be addressed in the \"Treating null/empty fields\" step.\n",
    "\n",
    "For now, the only action required in the 'Casting Types' step is the following:\n",
    "\n",
    "-   **`reviews_per_month`**: This will be converted from `string` to a numeric `double` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b83e0c68-f92b-4db6-9880-5e14f202543c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyzing null values"
    }
   },
   "outputs": [],
   "source": [
    "# getting total number of rows from the df_reviews table\n",
    "mappingNullvalues(df_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1581e26-8c3e-4f54-ba5e-715ef60fb3f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Analysis of Null Values**\n",
    "\n",
    "The null value analysis for the `df_reviews` DataFrame reveals a consistent pattern: all `review_scores_...` columns, along with `last_review`, `first_review`, and `reviews_per_month`, share a similar null rate of approximately **26%**.\n",
    "\n",
    "**Hypothesis:** We hypothesize that these nulls are not random errors, but are an expected and direct consequence of listings that have never received a review.\n",
    "\n",
    "**Decision & Action Plan:**\n",
    "Based on this understanding, no columns will be removed. Instead, the null values will be imputed in the next step, **\"4.3.3 Treating null/empty fields\"**.\n",
    "\n",
    "Since the `number_of_reviews` column is fully populated (no nulls). We can infer that nulls in all other review-related columns (scores, rates, dates) correspond directly to listings with zero reviews. Imputing these fields with `0` is therefore a logical and accurate way to represent the absence of review activity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f34af08-72ba-4748-9d5d-76dc8738adef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### 4.3.2 Casting types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b3d6228-493f-4b74-b403-602b5fe961e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# converting \"reviews_per_month\" from string to the double\n",
    "df_reviews = df_reviews.withColumn(\"reviews_per_month\",\n",
    "               f.col(\"reviews_per_month\").cast(\"double\")\n",
    ")\n",
    "df_reviews.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eafe75c-2077-439a-856c-89f8688f3950",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Statistics"
    }
   },
   "outputs": [],
   "source": [
    "# data statistics\n",
    "df_reviews.select(\"number_of_reviews\",\"reviews_per_month\",\"number_of_reviews_ltm\", \"number_of_reviews_ly\").summary(\"count\", \"mean\", \"stddev\", \"min\", \"50%\", \"max\").show()\n",
    "\n",
    "df_reviews.select(\"review_scores_rating\",\"review_scores_accuracy\",\"review_scores_cleanliness\").summary(\"count\", \"mean\", \"stddev\", \"min\", \"50%\", \"max\").show()\n",
    "\n",
    "df_reviews.select(\"review_scores_checkin\",\"review_scores_communication\",\"review_scores_location\",\"review_scores_value\").summary(\"count\", \"mean\", \"stddev\", \"min\", \"50%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4bdb3ac-d3a3-42aa-bab9-dea1afb04485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Key Insights from Statistical Analysis**\n",
    "\n",
    "The statistical summary of the review-related columns reveals two distinct and important data distributions, highlighting why the **median** is a more representative measure of central tendency than the mean for this dataset.\n",
    "\n",
    "**1. Review Counts (Right-Skewed Distribution):**\n",
    "For metrics like `number_of_reviews` and `number_of_reviews_ltm`, the **mean is significantly higher than the median** (e.g., a mean of 36 total reviews vs. a median of 6). This indicates a classic right-skewed distribution, often called a \"long tail.\" It shows that while a few \"superstar\" listings have an extremely high number of reviews, the vast majority of listings have very few. The median, therefore, better reflects the experience of a \"typical\" listing, which is not skewed by the highly popular outliers. Notably, the median of `0` for reviews in the last 12 months reveals that over half the listings had no recent review activity.\n",
    "\n",
    "**2. Review Scores (Left-Skewed Distribution):**\n",
    "Conversely, for all `review_scores_...` columns, the **median is consistently higher than the mean** (e.g., a median rating of 4.86 vs. a mean of 4.72). This indicates a left-skewed distribution, where most data points are clustered at the high end of the scale. It confirms that the typical guest experience is overwhelmingly positive. The mean is slightly pulled down by a small number of very low scores (outliers), making the median a more accurate representation of the consistently high satisfaction level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f4ba377-f159-4336-9d7b-35d18237ff02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####4.3.3 Treating null/empty fields\n",
    "\n",
    "**Hypothesis and Strategy**\n",
    "\n",
    "As established in the previous analysis, our primary hypothesis is that the null values in `reviews_per_month` and all `review_scores_...` columns are a direct result of the listing having zero reviews.\n",
    "\n",
    "To test this and treat the nulls simultaneously, we will perform a conditional imputation. For every row where the `number_of_reviews` column is `0`, we will fill the corresponding null values in all review-score and rate columns with the integer `0`. A value of `0` is a logical imputation, as it accurately reflects the absence of any review activity or score.\n",
    "\n",
    "**Verification**\n",
    "\n",
    "After this operation, we will re-run the null value analysis. If our hypothesis is correct, the null counts for these target columns should drop to zero, confirming that all missing review data was indeed linked to listings with no reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c8aa44-ba7e-4284-a8ea-5551b2a3208f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# corrected list of columns to apply the conditional fill\n",
    "columns_to_fill = [\n",
    "    \"reviews_per_month\",\n",
    "    \"review_scores_rating\",\n",
    "    \"review_scores_accuracy\",\n",
    "    \"review_scores_cleanliness\",\n",
    "    \"review_scores_checkin\",\n",
    "    \"review_scores_communication\",\n",
    "    \"review_scores_location\",\n",
    "    \"review_scores_value\"\n",
    "]\n",
    "\n",
    "# looping through the columns and apply the conditional fill\n",
    "for col_name in columns_to_fill:\n",
    "    df_reviews = df_reviews.withColumn(\n",
    "        col_name,\n",
    "        f.when(\n",
    "            f.col(\"number_of_reviews\") == 0, 0\n",
    "        ).otherwise(f.col(col_name))\n",
    "    )\n",
    "\n",
    "# analyzing nulls after imputation\n",
    "mappingNullvalues(df_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71747ce7-2b64-499e-9f72-2b9c633eaf0e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Statistics"
    }
   },
   "outputs": [],
   "source": [
    "# data statistics\n",
    "df_reviews.select(\"number_of_reviews\",\"reviews_per_month\",\"number_of_reviews_ltm\", \"number_of_reviews_ly\").summary(\"count\", \"mean\", \"stddev\", \"min\", \"50%\", \"max\").show()\n",
    "\n",
    "df_reviews.select(\"review_scores_rating\",\"review_scores_accuracy\",\"review_scores_cleanliness\").summary(\"count\", \"mean\", \"stddev\", \"min\", \"50%\", \"max\").show()\n",
    "\n",
    "df_reviews.select(\"review_scores_checkin\",\"review_scores_communication\",\"review_scores_location\",\"review_scores_value\").summary(\"count\", \"mean\", \"stddev\", \"min\", \"50%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "550026f8-ddce-467e-b7bb-e49c6bccaf1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Final Conclusions and Dashboarding Recommendations\n",
    "\n",
    "**1. Hypothesis Confirmation**\n",
    "\n",
    "The conditional imputation was highly successful. The null counts for `reviews_per_month` and all `review_scores_...` columns dropped from ~26% to nearly zero (a residual ~0.09% remains, indicating a separate minor data quality issue). This confirms our primary hypothesis: the vast majority of missing review data was directly linked to listings with `number_of_reviews == 0`.\n",
    "\n",
    "**2. Impact on Statistical Metrics**\n",
    "\n",
    "The imputation of `0` for these records has, as expected, significantly impacted the summary statistics:\n",
    "\n",
    "-   **The Mean:** A notable drop was observed in the mean for all imputed columns (e.g., the mean `review_scores_rating` fell from ~4.72 to ~3.49). This is not an error, but a positive outcome. The new mean provides a more **holistic and realistic** measure of the entire listing population, as it now includes the large segment of listings with zero review performance. The previous mean was biased, representing only the subset of listings that already had reviews.\n",
    "-   **The Median:** In contrast, the median for the review scores remained relatively high (e.g., the median `review_scores_rating` only shifted from ~4.86 to ~4.73). This is a powerful insight. It demonstrates that the **\"typical\" experience for a listing that *does* get reviewed is still excellent**.\n",
    "\n",
    "**3. Recommendations for Dashboard Representation**\n",
    "\n",
    "-   **Prioritize the Median for KPIs:** The median is the most honest and robust metric for representing a \"typical\" score, as it is unaffected by the large number of `0`-value outliers.\n",
    "-   **Contextualize the Mean:** Label the mean clearly to avoid misinterpretation (e.g., \"Overall Average Score, including listings with 0 reviews\"), as it represents the entire population.\n",
    "-   **Visualize the Bimodal Distribution:** Use a **histogram** to clearly show the two distinct groups: a large spike at `0` (the imputed listings) and another at the high end of the scale (4.5-5.0).\n",
    "-   **Implement a User-Driven Filter:** In Power BI, provide a **slicer** that allows users to toggle between viewing \"All Listings\" and only \"Actively Reviewed Listings.\"\n",
    "-   **Handle Null Dates:** Leave the remaining nulls in `last_review` and `first_review` as `NULL`. In the dashboard, handle this by creating a \"Not Reviewed\" category or by filtering them out of time-based analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8c23aa8f-8c51-4591-b543-8e9578de2de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 4.4 df_amenities\n",
    "\n",
    "This table contains a list of additional room amenities and can be used to investigate the pattern of successful rooms in New York.\n",
    "\n",
    "To facilitate analysis, we decided to create a table with 1 and 0 values ‚Äã‚Äãper column, representing whether the listing has that amenity or not. This can allow for simpler correlation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c8d1eff-81e7-49f4-9b37-d1e3dc3225c7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Type casting: df_amenities"
    }
   },
   "outputs": [],
   "source": [
    "# transforming strings to arrays\n",
    "df_amenities = df_amenities.withColumn(\"amenities_array\",\n",
    "               f.from_json(f.col(\"amenities\"),\n",
    "               t.ArrayType(t.StringType()))\n",
    ")\n",
    "df_amenities.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e11f882-f06b-47ed-b0cd-b6e9948759c3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get distinct amenities"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# counting distinct values in amenities column\n",
    "def extractDistinctValuesArray(df, array_column):\n",
    "    # exploding array column to get individual values\n",
    "    distinct_values = (df\n",
    "                    .select(f.explode(f.col(array_column)).alias(\"value\")) \n",
    "                    .filter(f.col(\"value\").isNotNull())\n",
    "                    .select(\"value\")\n",
    "                    .distinct()\n",
    "    )\n",
    "    # creating a list of distinct values getting them from df with collect()\n",
    "    amenities_distinct = []\n",
    "    for row in distinct_values.collect():\n",
    "        amenities_distinct.append(row['value'])\n",
    "    \n",
    "    return amenities_distinct\n",
    "\n",
    "amenities_distinct = extractDistinctValuesArray(df_amenities, \"amenities_array\")\n",
    "\n",
    "# printting list and its counting\n",
    "print(amenities_distinct)\n",
    "print(f\"Distinct amenities count: {len(amenities_distinct)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abc365ad-12ed-4f82-8e6b-28042fa6a5fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Observing the count and data, we notice that the amenities input is not predetermined and the user can write freely, which makes human analysis difficult.\n",
    "\n",
    "We'd like to know **how many amenities each room has** and **classify them into groups**. To do this, we'll use the `MLib` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6e90834-ac77-4943-9f95-9ed201aa55be",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importing libs"
    }
   },
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, HashingTF\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "424cb5e4-fec3-4260-9b60-8c746b9200f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Distinct amenities to df"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "We'll train a model to analyze the amenities data and organize it into groups.\n",
    " The idea is to get between 5 and 10 groups to create columns with.\n",
    "\n",
    "    1 - Tokenize and create 15 clusters, considering\n",
    "        that the model may repeat groups when clustering, so we\n",
    "        have some leeway.\n",
    "\n",
    "    2 - Check the quality of the clustering. If it's good,\n",
    "        we'll define the groups based on it.\n",
    "\n",
    "    3 - Create columns with the groups in question and fill\n",
    "        '1' when the list has amenities in that group and '0' when it doesn't.\n",
    "'''\n",
    "\n",
    "# creating schema for DataFrame\n",
    "schema = t.StructType([t.StructField(\"amenity\", t.StringType(), True)])\n",
    "\n",
    "# creating a DataFrame with amenities as rows\n",
    "df_sample = df_amenities.select(\"amenities_array\").limit(5000)\n",
    "df_exploded_distinct = df_sample.select(f.explode(\"amenities_array\").alias(\"amenity\")).distinct() #using distinct to permit analize clustering quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2acff3a8-1da5-4a49-abf8-b7b039983b6a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Pipeline creation"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# creating optimized pipeline for amenities clustering\n",
    "def creatingClusteringPipeline(groups_num=20):\n",
    "    # 1. Tokenization\n",
    "    tokenizer = Tokenizer(inputCol=\"amenity\", outputCol=\"words\")\n",
    "\n",
    "    # 2. stop words to remove\n",
    "    amen_stopwords = [\n",
    "        'with', 'and', 'or', 'the', 'a', 'an', 'in', 'on', 'at', 'to', \n",
    "        'for', 'of', 'by', 'via', '&', '‚Äì', 'available', 'specific', 'wih'\n",
    "    ]\n",
    "\n",
    "    stopwords_remover = StopWordsRemover(\n",
    "        inputCol=\"words\",\n",
    "        outputCol=\"filtered_words\",\n",
    "        stopWords=amen_stopwords\n",
    "    )\n",
    "    \n",
    "     # 3. mapping words with a limited number of features (1500)\n",
    "    hashing_tf = HashingTF(\n",
    "        inputCol=\"filtered_words\",\n",
    "        outputCol=\"features\",\n",
    "        numFeatures=1500,            \n",
    "    )\n",
    "    \n",
    "    # 4. dataframe clustering with groups_num criteria\n",
    "    kmeans = KMeans(\n",
    "        k=groups_num,\n",
    "        featuresCol=\"features\",\n",
    "        predictionCol=\"group\",\n",
    "        seed=42,              # keeping the same seed for reproducibility\n",
    "        maxIter=30,           # rising the number of iterations for better results\n",
    "        tol=1e-6\n",
    "        )\n",
    "    \n",
    "    pipeline = Pipeline(stages=[tokenizer, stopwords_remover, hashing_tf, kmeans])\n",
    "    return pipeline\n",
    "\n",
    "# Creating and training pipeline\n",
    "pipeline = creatingClusteringPipeline(groups_num=20)\n",
    "model = pipeline.fit(df_exploded_distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb9cbe3d-36e4-492f-8e16-6d45b17b9c0c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Naming suggestion for clusters"
    }
   },
   "outputs": [],
   "source": [
    "# creating a function to suggest group names based on the most frequent words\n",
    "def suggestGroupNaming(amenities_list):\n",
    "    # keywords by categories\n",
    "    categories = {\n",
    "        'TV & Streaming': ['tv', 'hdtv', 'netflix', 'hbo', 'disney', 'hulu', 'prime', 'video', 'streaming','chromecast', 'roku', 'apple'],\n",
    "        'Gaming & Entertainment': ['game', 'console', 'xbox', 'playstation', 'nintendo', 'switch', 'ps4', 'ps5', 'dvd', 'bluray'],\n",
    "        'Internet & Connectivity': ['wifi', 'internet', 'mbps', 'ethernet', 'broadband'],\n",
    "        'Climate Control': ['ac', 'air', 'heater', 'fan', 'ventilation'],\n",
    "        'Kitchen Appliances': ['refrigerator', 'fridge', 'stove','oven','microwave', 'keurig', 'toaster','kettle','dishwasher', 'cooktop', 'induction'],\n",
    "        'Coffee & Beverage': ['coffee', 'espresso', 'maker', 'french', 'press', 'nespresso'],\n",
    "        'Bathroom Amenities': ['shampoo', 'hair', 'conditioner', 'soap', 'body', 'shower', 'bidet', 'towel', 'hairdryer'],\n",
    "        'Laundry Facilities': ['washer', 'dryer', 'laundry', 'iron', 'laundromat', 'drying'],\n",
    "        'Parking & Transportation': ['parking', 'garage', 'valet', 'street', 'free'],\n",
    "        'Security & Access': ['keypad', 'lock', 'security', 'alarm', 'camera', 'safe', 'detector'],\n",
    "        'Fitness': ['gym', 'exercise', 'weights', 'treadmill', 'yoga', 'elliptical', \"exercise\", 'equipment', 'workout'],\n",
    "        'Family & Kids': ['high' 'chair', 'crib', 'children', 'toys', 'baby', 'monitor'],\n",
    "        'Outdoor Spaces': ['patio', 'balcony', 'terrace', 'garden', 'outdoor', 'waterfront', 'view'],\n",
    "        'Outdoor recreation': ['grill', 'firepit', 'fire', 'bbq', 'grill', 'pool','swimming','hot', 'tub', 'sauna', 'jacuzzi'],\n",
    "        'Sound Systems': ['sound', 'system', 'speaker', 'bluetooth', 'sonos', 'bose', 'jbl', 'marshall', 'audio'],\n",
    "    }\n",
    "\n",
    "    # counting matches by category\n",
    "    categories_count = {}\n",
    "    full_text = ' '.join(amenities_list).lower()\n",
    "    \n",
    "    for cat, keywords in categories.items():\n",
    "        matches = sum(1 for word in keywords if word in full_text)\n",
    "        if matches > 0:\n",
    "            categories_count[cat] = matches\n",
    "    \n",
    "    if categories_count:\n",
    "        main_categorie = max(categories_count.items(), key=lambda x: x[1])[0]\n",
    "        return main_categorie\n",
    "    else:\n",
    "        # if no category is found, return a generic name\n",
    "        return f\"Generic group ({len(amenities_list)} amenities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7060bcf-a26f-4ab6-8ace-46a99f6a2ff4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Naming clusters according to criteria"
    }
   },
   "outputs": [],
   "source": [
    "# analizing clusters to create groups by looking at the most frequent words\n",
    "def namingGroups(model, df_distinct, num_groups=15):\n",
    "    # 1. Applying model to clusters creation \n",
    "    df_with_groups = model.transform(df_exploded_distinct)\n",
    "    \n",
    "    # 2. Analizing and naming clusters based on the most frequent words\n",
    "    named_groups = {}\n",
    "    \n",
    "    for group_id in range(num_groups):\n",
    "        grouped_amenities = df_with_groups.filter(f.col(\"group\") == group_id\n",
    "                                                  ).select(\"amenity\"\n",
    "                                                  ).collect()\n",
    "        \n",
    "        amenities_list = [row['amenity'] for row in grouped_amenities]\n",
    "        \n",
    "        # analizing patterns\n",
    "        group_name = suggestGroupNaming(amenities_list)\n",
    "        \n",
    "        named_groups[group_id] = {\n",
    "            'name': group_name,\n",
    "            'amenities': amenities_list,\n",
    "            'qtd': len(amenities_list)\n",
    "        }\n",
    "        \n",
    "        # print for visualization\n",
    "        print(f\"{group_id}: {group_name}\")\n",
    "        print(f\"{len(amenities_list)} amenities\")\n",
    "        print(f\"ex.: {amenities_list[:10]}\")\n",
    "        print()\n",
    "    \n",
    "    return named_groups\n",
    "\n",
    "# calling nameing function\n",
    "named_groups = namingGroups(model, df_exploded_distinct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77c0d868-fd58-4cca-bb75-f2f7eea32cea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Creating colunms"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "There are 2 poorly classified groups, let's ignore them and keep the other groups that has mor accurate data\n",
    "'''\n",
    "\n",
    "remove_groups = [11, 13]\n",
    "named_groups = {\n",
    "    group_id: group_info\n",
    "    for group_id, group_info in named_groups.items()\n",
    "    if group_id not in remove_groups\n",
    "}\n",
    "\n",
    "# creating a new column with amenities list size\n",
    "df_amenities = df_amenities.withColumn('amenities_count',\n",
    "                            f.size(f.col('amenities_array'))\n",
    "                        )\n",
    "\n",
    "# 1. Extracting unique group names and create mapping\n",
    "group_names = list(set([info['name'] for info in named_groups.values()]))\n",
    "\n",
    "# 2. Associating each amenity with its group\n",
    "amenity_to_group = {}\n",
    "for group_id, group_info in named_groups.items():\n",
    "    group_name = group_info['name']\n",
    "    for amenity in group_info['amenities']:\n",
    "        if amenity not in amenity_to_group:\n",
    "            amenity_to_group[amenity] = set()\n",
    "\n",
    "        amenity_to_group[amenity].add(group_name) \n",
    "    \n",
    "# 3. Creating a function to check if any amenity belongs to the target group\n",
    "def checkAmenityGroup(amenities_list, target_group):\n",
    "    if amenities_list is None:\n",
    "        return 0\n",
    "    \n",
    "    for amenity in amenities_list:\n",
    "        if amenity in amenity_to_group and target_group in amenity_to_group[amenity]:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# 4: register UDF for each group\n",
    "for group_name in group_names:\n",
    "    # Create UDF for the target group\n",
    "    check_group_udf = udf(\n",
    "                      lambda amenities: checkAmenityGroup(amenities, group_name),\n",
    "                      t.IntegerType()\n",
    "                )\n",
    "\n",
    "    # adding a column for the target group\n",
    "    column_name = group_name.replace(\" & \", \"_\"\n",
    "                                     ).replace(\" \", \"_\"\n",
    "                                     ).replace(\"-\", \"_\")\n",
    "\n",
    "    df_amenities = df_amenities.withColumn(column_name,\n",
    "                                check_group_udf(f.col(\"amenities_array\"))\n",
    "        )\n",
    "    \n",
    "# 5. Selecting the final columns\n",
    "col_name = [group_name.replace(\" & \", \"_\"\n",
    "                        ).replace(\" \", \"_\"\n",
    "                        ).replace(\"-\", \"_\") \n",
    "                        for group_name in group_names\n",
    "                    ]\n",
    "    \n",
    "final_columns = [\"room_id\"] + col_name + [\"amenities_count\"]\n",
    "    \n",
    "df_amen_final = df_amenities.select(final_columns)\n",
    "    \n",
    "for group_name in group_names:\n",
    "    col_name = group_name.replace(\" & \", \"_\"\n",
    "                        ).replace(\" \", \"_\"\n",
    "                        ).replace(\"-\", \"_\"\n",
    "                    )\n",
    "    count_ones = df_amen_final.filter(f.col(col_name) == 1).count()\n",
    "    total_rows = df_amen_final.count()\n",
    "    percentage = (count_ones / total_rows) * 100\n",
    "    print(f\"   {col_name}: {count_ones}/{total_rows} rooms ({percentage:.1f}%)\")\n",
    "\n",
    "df_amen_final.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d463659-741d-4e75-af9b-9a5cb9619031",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###5. Saving DataFrames to Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c246a23-f21f-4dd7-987b-a0b9f77140c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE SCHEMA IF NOT EXISTS workspace.projeto_airbnb;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e37f1e11-064d-4ac7-9db1-415c42086cc9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Exporting dataframes"
    }
   },
   "outputs": [],
   "source": [
    "# define catalog and schema names\n",
    "catalog_name = \"workspace\"\n",
    "schema_name = \"projeto_airbnb\"\n",
    "\n",
    "# save each DataFrame as a table in Delta format (default)\n",
    "df_rooms.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{catalog_name}.{schema_name}.rooms\")\n",
    "df_hosts.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{catalog_name}.{schema_name}.hosts\")\n",
    "df_reviews.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{catalog_name}.{schema_name}.reviews\")\n",
    "df_amen_final.write.mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(f\"{catalog_name}.{schema_name}.amenities\")\n",
    "\n",
    "print(\"All tables saved successfully in Unity Catalog!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8071791453516798,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "AirBNB New York Revenue Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
